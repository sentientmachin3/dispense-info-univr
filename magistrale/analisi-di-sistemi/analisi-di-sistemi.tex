\documentclass[a4paper, 10pt]{book}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{frontespizio}
\usepackage{hyperref}
\hypersetup{hidelinks,
	colorlinks = true,
	urlcolor = black, 
	linkcolor = black}
\usepackage[margin=3cm]{geometry}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{stmaryrd}
\usepackage[strict]{changepage}
\usepackage{galois}
\usepackage{multicol}
\usepackage{float}
\usepackage{textcomp}
\usepackage{makecell}


\lstset{tabsize=2,
	basicstyle=\small\ttfamily}

\newcommand{\code}[1]{\textup{\lstinline{#1}}}
\newcommand{\parts}[1]{\mathcal{P}(#1)}
\newcommand{\galoistuple}{\langle C, \alpha, \gamma , A \rangle}
\newcommand{\sem}[1]{\llbracket #1 \rrbracket (\rho, \mu)}

\newtheorem{definit}{Definizione}[subsection]
\newtheorem{thm}{Teorema}[subsection]


\begin{document}
	\begin{frontespizio}
		\Preambolo{\usepackage{datetime}}
		\Istituzione{Università degli Studi di Verona}
		\Divisione{Dipartimento di informatica}
		\Titolo{Analisi di Sistemi informatici}
		\Scuola{}
		\Sottotitolo{Riassunto dei principali argomenti}
		\Candidato{Davide Bianchi}
		\Candidato{Marco Colognese}
		\Candidato{Mattia Rossini}
		\NCandidato{Autori}
		\Annoaccademico{2017/2018}
	\end{frontespizio}
	
	\tableofcontents
	\newpage


	
	\chapter{Interpretazione astratta}
	\section{Introduzione}
	Lo scopo è quello di trovare un'approssimazione di una semantica $\langle P \rangle$ di $\llbracket P \rrbracket$ tale per cui valgano:
	\begin{itemize}
		\item \textit{correttezza:} $\llbracket P \rrbracket \subseteq \langle P \rangle$;
		\item \textit{decidibilità:} $\langle P \rangle \subseteq Q$ è decidibile ($Q$ è un insieme di semantiche che soddisfa la proprietà di interesse).
	\end{itemize}

	Se entrambe le proprietà sono soddisfatte, allora vale che \[ (\langle P \rangle \subseteq Q) \Rightarrow (\llbracket P \rrbracket \subseteq Q) \]
	
	La semantica è data da una coppia $\langle D, f \rangle$ dove $D$ è una coppia $\langle D, \leq_D$ rappresentante un dominio semantico e $f: D \to D$ è una funzione di trasferimento con una soluzione a punto fisso.
	
	Dato un oggetto concreto, definiamo:
	\begin{itemize}
		\item un \textbf{oggetto astratto} come una rappresentazione matematica sovra-approssimata del corrispondente concreto;
		\item un \textbf{dominio astratto} come un insieme di oggetti astratti con delle operazioni astratte, che approssimano quelle concrete;
		\item una funzione di \textbf{astrazione} $\alpha$ che mappa oggetti concreti in oggetti astratti;
		\item una funzione di \textbf{concretizzazione} $\gamma$ che mappa oggetti astratti in oggetti concreti.
	\end{itemize}
	
	La caratteristica peculiare delle astrazioni è che solo alcune proprietà vengono osservate con esattezza, le altre vengono solo approssimate. In sostanza, dato un dominio astratto $A$, gli elementi di $A$ sono osservati con esattezza, gli altri sono approssimati o l'informazione è persa del tutto.
	
	\paragraph{Proprietà.} L'insieme delle proprietà $\parts{\Sigma}$ di oggetti in $\Sigma$ è l'insieme di elementi che gode di quella proprietà. Questo insieme di proprietà costituisce un reticolo completo \[ \langle \parts{\Sigma}, \subseteq, \emptyset, \cup, \cap, \neg \rangle \] dove:
	\begin{itemize}
		\item $\subseteq$ è l'implicazione logica;
		\item $\Sigma$ è \verb|true|;
		\item $\cup$ è la disgiunzione (oggetti che godono di $P$ o di $Q$ appartengono a $P \cup Q$);
		\item $\cap$ è la congiunzione (oggetti che godono di $P$ e di $Q$ appartengono a $P \cap Q$);
		\item $\neg$ è la negazione (oggetti che non godono di $P$ stanno in $\Sigma \setminus P$).
	\end{itemize}

	\paragraph{Direzione dell'astrazione.}
	Quando si approssima una proprietà concreta $P \in \parts{\Sigma}$ usando una proprietà astratta $\overline{P}$, deve essere stabilito un criterio per definire quando $\overline{P}$ è un'approssimazione di $P$.
	
	Si distinguono quindi i seguenti casi:
	\begin{itemize}
		\item approssimazione \textit{da sopra}: $P \subseteq \overline{P}$;
		\item approssimazione \textit{da sotto}: $P \supseteq \overline{P}$.
	\end{itemize}
	
	Dato un oggetto $o$, si vuole quindi sapere se $o \in P$:
	\begin{align*}
		P \supseteq \overline{P}: \begin{cases}
			\text{"Si"} &o \in \overline{P} \\
			\text{"Non lo so"} &o \notin  \overline{P}
		\end{cases} \qquad
		P \subseteq \overline{P}: \begin{cases}
		\text{"No"} &o \notin \overline{P} \\
		\text{"Non lo so"} &o \in \overline{P}\\
		\end{cases}
	\end{align*} 
	
	\paragraph{Migliore approssimazione.}
	Definiamo come \textit{migliore approssimazione} di una proprietà $P$ in $A$ il glb delle over-approximation di $P$ in $A$, ossia: \[  \overline{P} = \bigcap \lbrace \overline{P'} \in A | P \subseteq \overline{P'} \rbrace \in A \]
	
	\section{Connessione di Galois}
	Imponiamo il vincolo che $\alpha$ e $\gamma$ siano monotone, allora concludiamo che: \begin{itemize}
		\item $\gamma \circ \alpha: C \to C$ è \textbf{estensiva}: $\gamma(\alpha(c)) \geq c$;
		\item $\alpha \circ \gamma : A \to A$ è \textbf{riduttiva}: $\alpha(\gamma(a)) \leq a$.
	\end{itemize}

	Le definizioni qui sopra dicono rispettivamente che:
	\begin{itemize}
		\item $\alpha$ perde informazione, e $\gamma$ non la può recuperare;
		\item $\gamma$ non perde informazione.
	\end{itemize}

	\begin{definit}[Connessione di Galois]
		Dati due poset $\langle A , \leq_A \rangle$ e $\langle C , \leq_C \rangle$, e due funzioni monotone $\alpha: C \to A$ e $\gamma: A \to C$, diciamo che $\galoistuple$ è una connessione di Galois se:
		\begin{itemize}
			\item $\forall c \in \mathcal{C}: c \leq_C \gamma(\alpha(c))$
			\item $\forall a \in \mathcal{A}: \alpha(\gamma(a)) \leq_A a$
		\end{itemize}
		
		Se inoltre vale che $\forall a \in \mathcal{A}: \alpha(\gamma(a)) = a$, allora $\galoistuple$ è un'inserzione di Galois.
	\end{definit}
	Una connessione e un'inserzione di Galois sono rappresentate rispettivamente come \[  C \galois{\alpha}{\gamma} A \qquad C \galoiS{\alpha}{\gamma} A \]
	La funzione $\alpha$ è detta \textit{aggiunta sinistra}, mentre la funzione $\gamma$ è detta \textit{aggiunta destra}.
	
	\begin{thm}
		Data una connessione di Galois $ C \galois{\alpha}{\gamma} A$, sono equivalenti:
		\begin{itemize}
			\item $C \galoiS{\alpha}{\gamma} A$;
			\item $\alpha$ è suriettiva;
			\item $\gamma$ è iniettiva.
		\end{itemize}
	\end{thm}

	Inoltre, dati due domini astratti, non esistono due coppie $(\alpha, \gamma)$ che formino una connessione di Galois; quindi la connessione di Galois tra due domini è \textbf{unica}, e le funzioni sono identificabili attraverso:
	\begin{align*}
		\alpha(c) &= \bigwedge \lbrace a \in A \vert c \leq_C \gamma(a) \rbrace \\
		\gamma(a) &= \bigvee \lbrace c \in C \vert \alpha(c) \leq_A a \rbrace
	\end{align*}
	
	\section{Famiglie di Moore}
	\begin{definit}[Famiglia di Moore]
		Sia $L$ un reticolo completo. $X \subseteq L$ è una famiglia di Moore di $L$ se \[ X = \mathcal{M}(X) = \Big\{ \bigwedge S\ \vert\ S \subseteq X \Big\} \] dove \[ \bigwedge \emptyset = \top \in \mathcal{M}(X) \]
	\end{definit}
	
	Da questa definizione segue che, ipotizzando che ogni proprietà concreta abbia una migliore astrazione $\overline{P} \in A$, implica che il dominio $A$ è una famiglia di Moore.
	
	\section{Upper closure operator}
	\begin{definit}[Upper closure operator]
		Una funzione $f:P \to P$ su un poset $\langle P, \leq_P \rangle $ è un upper closure operator (uco) se soddisfa le seguenti proprietà:\begin{itemize}
			\item estensività: $\forall x \in P: x \leq_P \rho(x)$
			\item monotonia: $\forall x,y \in P: (x \leq_P y) \Rightarrow (\rho(x) \leq_P \rho(y)$
			\item idempotenza: $\forall x \in P: \rho(x) = \rho(\rho(x))$
		\end{itemize}
	\end{definit}

	I lower closure operator sono definiti in modo duale, specificando che $\rho$ deve essere \textit{riduttiva}, ovvero che $\forall x \in P: x \geq_P \rho(x)$.
	
	\begin{thm}
		Data una connessione di Galois $C \galois{\alpha}{\gamma} A$ si ha che $\gamma \circ \alpha$ è un uco e $\alpha \circ \gamma $ è un lco.
	\end{thm}
	\begin{thm}
		$C \galoiS{\alpha}{\gamma} A$ se e solo se $A$ è isomorfo \footnote{Con isomorofismo si intendono reticoli con la stessa struttura.} ad una Moore family di $C$.
	\end{thm}
	\begin{thm}
		Sia $\rho \in uco(c)$. Allora $\forall A \simeq \rho(C)$ si ha che $\exists \alpha, \gamma : C \galoiS{\alpha}{\gamma} A$
	\end{thm}
	
	\section{Reticolo delle interpretazioni astratte}
	I vari domini astratti possono essere comparati sulla base della loro precisione. In generale si può dire che un dominio astratto $A_1$ è più preciso di $A_2$ (indicato attraverso $A_1 \sqsubseteq A_2$) quando \[ \forall a_2 \in A_2, \exists a_1 \in A_1 \quad \text{ tali che } \quad \gamma_1(a_1) = \gamma_2(a_2) \] ovvero quando \[ \gamma(A_2) \subseteq \gamma(A_1) \]
	
	Collegando agli uco, possiamo dire che \[ A_1 \sqsubseteq A_2 \Leftrightarrow \rho_1 \sqsubseteq \rho_2 \Leftrightarrow \rho_2(C) \subseteq \rho_1(C) \]
	
	\begin{definit}[Reticolo delle int. astratte]
		Se $C$ è un reticolo completo o un cpo, allora \[ \langle uco(C), \sqsubseteq, \sqcup, \sqcap, \lambda x.\top, \lambda x.x \rangle \] è un reticolo completo dove $\forall \rho, \eta \in uco(C), \{ \rho_i \}_{i \in I} \subseteq uco(C)$ e $x \in C$:\begin{itemize}
			\item $\rho \sqsubseteq \eta \Leftrightarrow \forall y \in C.\rho(y) \leq \eta(y) \Leftrightarrow \eta(C) \subseteq \rho(C)$
			
			\item $ \displaystyle \Big( \bigsqcap_{i \in I} \rho_i \Big)(x) = \bigwedge_{i \in I} \rho_i(x)$
			
			\item $ \displaystyle \Big( \bigsqcup_{i \in I} \rho_i \Big)(x) = x \Leftrightarrow \forall i \in I. \rho_i(x)=x$
			
			\item $\lambda x.\top, \lambda x.x$ sono rispettivamente top e bottom. 
		\end{itemize}
	\end{definit}

	\section{Computazioni astratte e concrete}
	\begin{definit}[Correttezza]
		Data un'inserzione di Galois $C \galoiS{\alpha}{\gamma} A$, una funzione concreta $f: C \to C$ e una funzione astratta $f^\sharp: A \to A$ diciamo che $f^\sharp$ è un'approssimazione corretta di $f$ se \[ \forall c \in C: \alpha(f(c)) \leq_A f^\sharp(\alpha(c)) \quad \text{backward} \] o equivalentemente \[ \forall a \in A: f(\gamma(a)) \leq_C \gamma(f^\sharp(a) \quad \text{forward} \]
	\end{definit}
	
	Rinforzando la definizione e imponendo uguaglianza si perde l'equivalenza delle due espressioni sopra. 
	\begin{definit}[Completezza]
		Data un'inserzione di Galois $C \galoiS{\alpha}{\gamma} A$, una funzione concreta $f: C \to C$ e una funzione astratta $f^\sharp: A \to A$ diciamo che $f^\sharp$ è: \begin{itemize}
			\item backward-completa per $f$ se  $\forall c \in C: \alpha(f(c)) = f^\sharp(\alpha(c))$
			\item forward-completa per $f$ se $\forall a \in A: f(\gamma(a)) = \gamma(f^\sharp(a)$
		\end{itemize}
	\end{definit}
	
	La definizione rappresenta una situazione ideale in cui non si ha perdita di precisione durante il calcolo astratto. Inoltre la backward-completezza lavora sull'astrazione dell'input delle operazioni, la forward-completezza sull'output.
	
	Le definizioni di completezza possono essere date anche usando gli uco: \begin{itemize}
		\item $\rho \in uco(C)$ è backward-completo per $f$ se $\rho \circ f = \rho \circ f \circ \rho$
		\item $\rho \in uco(C)$ è forward-completo per $f$ se $f \circ \rho = \rho \circ f \circ \rho$
	\end{itemize}

	Inoltre quando $\rho$ è sia backward che forward-completo allora vale che $\rho \circ f = f \circ \rho$.
	
	\begin{thm}
		Data $C \galoiS{\alpha}{\gamma} A$, una funzione concreta $f : C \to C$  e una funzione astratta $f^\sharp: A \to A$ allora  \[ \forall c \in C: \alpha(f(c)) \leq_A f^\sharp(\alpha(c)) \Leftrightarrow \alpha \circ f \circ \gamma \sqsubseteq f^\sharp \]
	\end{thm}

	\begin{definit}[Best correct approximation]
		Data $C \galoiS{\alpha}{\gamma} A$ e una funzione concreta $f : C \to C$ allora $\alpha \circ f \circ \gamma : A \to A$ è la best correct approximation di $f$ in $A$.
	\end{definit}
	
	\section{Correttezza}
	Consideriamo $C \galois{\alpha}{\gamma} A$, una funzione concreta $f: C\rightarrow C$ e una funzione astratta $f^{\sharp}: A\rightarrow A$. Possiamo dire che $f^{\sharp}$ è un'approssimazione corretta di f in A se:
	\begin{equation*}
	\forall c\in C: \alpha(f(c))\leq_A f^{\sharp}(\alpha(c))
	\end{equation*}
	
	\noindent
	oppure, equivalentemente:
	\begin{equation*}
	\forall a\in A: f(\gamma(a))\leq_C \gamma(f^{\sharp}(a))
	\end{equation*}
	
	Nel processo di astrazione è ammessa una perdita di informazioni, ciò non è possibile nel processo di concretizzazione, dunque possiamo dire che se $c\in C$. Possiamo  dire che $\alpha(c)$ è l'elemento astratto più preciso che rappresenta \textit{c}.
	
	\begin{multicols}{2}	
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.35]{pngs/Correttezza}
			\caption{Condizione di correttezza: $\alpha(f(c))\leq_A f^{\sharp}(\alpha(c))$}
			\label{Correct}
		\end{figure}
		\columnbreak
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.35]{pngs/CorrettezzaAstr}
			\caption{Condizione di correttezza: $f(\gamma(a))\leq_C \gamma(f^{\sharp}(a))$}
			\label{CorrectAbstr}
		\end{figure}
	\end{multicols}
	
	\section{Completezza}
	Consideriamo $C \galoiS{\alpha}{\gamma} A$, una funzione concreta $f: C\rightarrow C$ e una funzione astratta $f^{\sharp}: A\rightarrow A$. Possiamo dire che:
	\begin{itemize}
		\item $f^{\sharp}$ è backward-complete per f se: $\forall c\in C: \alpha(f(c))=f^{\sharp}(\alpha(c))$;
		\item $f^{\sharp}$ è forward-complete per f se: $\forall a\in A: f(\gamma(a))=\gamma(f^{\sharp}(a))$.
	\end{itemize}
	
	I due tipi di completezza rappresentano una situazione in cui non si verifica nessuna perdita di precisione durante l'astrazione. In particolare:
	\begin{itemize}
		\item La \textbf{B}-completezza considera l'astrazione sull'output delle operazioni e non si accumula nessuna perdita di precisione astraendo in \textit{p} gli argomenti di \textit{f};
		\item La \textbf{F}-completezza considera l'astrazione sull'input delle operazioni e non si accumula nessuna perdita di precisione approssimando il risultato della funzione \textit{f} calcolata in \textit{p}.
	\end{itemize}
	
	\begin{multicols}{2}	
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.33]{pngs/Backward}
			\caption{Condizione di \textit{\textbf{B}-completezza}}
			\label{Backward}
		\end{figure}
		\columnbreak
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.33]{pngs/Forward}
			\caption{Condizione di \textit{\textbf{F}-completezza}}
			\label{Forward}
		\end{figure}
	\end{multicols}

	\section{Accelerazione della convergenza}
	\subsection{Widening}
	Un widening \[ \nabla : P \times P \to P  \] su un poset $ \langle P, \leq_P \rangle $ è una funzione che soddisfa:
	\begin{itemize}
		\item $\forall x,y \in P : x \sqsubseteq (x \nabla y ) \wedge y \sqsubseteq (x \nabla y)$
		\item per ogni catena ascendente $x_0 \sqsubseteq x_1 \sqsubseteq ... \sqsubseteq x_n$ la catena definita come $y_0 = x_0, ..., y_{n+1} = y_n \nabla x_{n+1}$ non è strettamente crescente.
	\end{itemize}
	
	Dato che in interpretazione astratta è necessario garantire/accelerare la convergenza, viene usato il widening (che si sostituisce al least upper bound), dal momento che anche il calcolo astratto può divergere.
	Il risultato di un widening è un post-puntofisso  di $F^\nabla$, ovvero una sovra-approssimazione del punto fisso più piccolo di f $lfp^\sqsubseteq F$. 
	
	Ad esempio, il widening su intervalli funziona come segue:
	\begin{align*}
	\lbrack a, b \rbrack\ \nabla\ \lbrack c, d \rbrack = \lbrack e, f \rbrack \qquad \text{ tale che}
	\end{align*}
	
	\begin{align*}
	e = 
	\begin{cases}
	-\infty &\text{ se } c < a \\
	a &\text{ altrimenti}
	\end{cases}
	\text{ e } f = 
	\begin{cases}
	+\infty &\text{ se } b < d\\
	b &\text{ altrimenti }
	\end{cases}
	\end{align*}
	
	\subsection{Narrowing}
	Dato che il widening raggiunge un post-fixpoint, piuò capitare che si abbiano eccessive perdite di informazione, in questo caso viene usato il narrowing.
	
	\begin{definit}
		Il narrowing è una funzione $ \triangle : P \times P \to P$ tale che:
		\begin{itemize}
			\item $\forall x, y \in \mathcal{P}: y \leq x \implies y \leq x\ \triangle\ y \leq x$
			\item Per ogni catena discendente $x_0 \geq x_1 \geq ...$, la catena discendente $y_0=x_0, ..., y_{i+1} = y_i\ \triangle\ x_{i+1}$ non è strettamente decrescente.
		\end{itemize}
	\end{definit}

	Per gli intervalli il narrowing funziona come segue:
	\begin{align*}
	\lbrack a, b \rbrack\ \triangle\ \lbrack c, d \rbrack = \lbrack e, f \rbrack \qquad \text{ tale che}
	\end{align*}
	\begin{align*}
	e = 
	\begin{cases}
	c &\text{ se } a = -\infty \\
	a &\text{ altrimenti}
	\end{cases}
	\text{ e } f = 
	\begin{cases}
	d &\text{ se } b = +\infty\\
	b &\text{ altrimenti }
	\end{cases}
	\end{align*}
	

\section{Linguaggio e semantica}
Introduciamo in questa sezione il linguaggio che verrà usato nel resto della dispensa e la sua semantica.

\begin{center}
	\begin{tabular}{cc}
		\hline
		\textbf{Statement} & \textbf{Codice} \\
		\hline
		\hline
		Variabili & \lstinline|x| \\
		Espressioni aritmetiche & \lstinline|e| \\
		Assegnamenti & $x \leftarrow e$ \\
		Lettura da memoria & $x \leftarrow M[e]$ \\
		Scrittura in memoria & $M[e]_1 \leftarrow e_2$ \\
		Condizionali & \lstinline|if (e) S|$_1$ \lstinline|else S|$_2$ \\
		Salto non condizionale & \lstinline|goto L|\\
		\hline
	\end{tabular}
\end{center}

La memoria $M$ è vista come un array arbitrariamente grande dove i valori possono essere inseriti e letti.

\begin{itemize}
	\item $x$ e $M[e]$ sono contenitori di valori;
	\item il contenuto di $M[e]$ non è visibile fino alla valutazione di $e$;
	\item $x$ è solamente il nome tramite cui accedere al contenitore associato.
\end{itemize}



\subsection{Collecting Semantics}
\MakeUppercase{è} l'insieme dei comportamenti osservabili nella semantica operazionale. La \textit{Collecting Semantics} è il punto di partenza per ogni tipo di analisi (non ne esiste una universale).

La \textbf{\textit{trace semantics}} di un programma accumula informazioni temporali riguardo l'esecuzione: una traccia tiene conto dell'ordine in cui i \textit{program states} sono raggiunti durante l'esecuzione.
\noindent
Le tracce analizzate possono essere dei seguenti tipi:
\begin{itemize}
	\item L'insieme di tutti i discendenti dello stato iniziale.
	\item L'insieme di tutti i discendenti dello stato iniziale che può raggiungere uno stato finale.
	\item Lo stato di tutte le tracce finite dallo stato iniziale.
	\item L'insieme di tutte le tracce infinite e finite dallo stato iniziale ecc.
\end{itemize}

Però non sempre siamo interessati alle informazioni temporali ma solamente agli invarianti presenti ad ogni \textit{program point}. Questi invarianti possono essere astratti dalle informazioni temporali attraverso la \textbf{\textit{collecting semantics}}.

Più formalmente, un invariante del programma $P$ al punto di programma $l$ è una qualsiasi proprietà $I\in P$ (store) che è presente ogni talvolta che $l$ viene raggiunto.

La \textit{collecting semantics} di $P$ è semplicemente l'associazione tra i vari \textit{program point} e le corrispondenti invarianti ben precise.

Lo stato di input non è noto al momento della compilazione, quindi vengono collezionati tutti gli stati raggiungibili da tutti i possibili ingressi del programma.
\newline

Si tratta di una collezione di stati che possono apparire su alcune tracce nei diversi program point. Trattandosi di un'astrazione, non è più possibile risalire alle tracce di esecuzione del programma conoscendo solamente i vari \textit{program states}.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{pngs/TraceCollect}
	\caption{Esiste la traccia rossa? \textit{Trace semantics}: NO; \textit{collecting semantics}: NON LO SO.}
\end{figure}



\section{Control-Flow-Graph (CFG)}
E costituito da:
\begin{itemize}
	\item \textbf{nodi}: corrispondono ai \textit{program points};
	\item \textbf{archi}: passi di computazione etichettati con la corrispondente azione; sono della forma $K=(u, lab, v)$, dove $u$ è il nodo sorgente, $v$ è il nodo di destinazione e $lab$ è l'etichetta.
\end{itemize}	
\begin{center}
	\begin{tabular}{cc}
		\hline
		Test & $NonZero(e)$ or $Zero(e)$ \\
		Assegnamenti & $x \leftarrow e$ \\
		Lettura da memoria & $x \leftarrow M[e]$ \\
		Scrittura in memoria & $M[e]_1 \leftarrow e_2$ \\
		Statement vuoto & $;$ \\
		\hline
	\end{tabular}
\end{center}

Ogni passo di computazione della semantica operazionale trasforma gli stati del programma:
\begin{equation*}
	(\rho, \mu) \textnormal{ dove } \rho: Var \to int \textnormal{ e } \mu : \mathbb{N} \to int
\end{equation*}
\begin{itemize}
	\item La funzione $\rho$ mappa le variabili del programma al loro valore attuale;
	\item la funzione $\mu$ mappa ogni cella dell'array al suo contenuto nelle celle di memoria.
\end{itemize}

Una \textit{computazione} è un percorso che và da un nodo di partenza $u$ e termina in un nodo $v$. Il percorso è un insieme di archi del $CFG$. La trasformazione dello stato è data dalla composizione degli effetti degli archi.

\begin{equation*}
	\llbracket \pi\rrbracket\hspace{0.01cm} = \llbracket k_n\rrbracket \circ ... \circ \llbracket k_1\rrbracket
\end{equation*}

Il \textbf{Control Flow Graph} è generato dalla sintassi del programma ed è utile per capire la struttura del codice.

Viene utilizzato per effettuare \textit{debugging}, \textit{testing} ed individuare \textit{dead code}.

\newtheorem*{definit1}{Basic Block}
\begin{definit1}
	Sequenza massima di statements consecutivi con un singolo \textit{entry point}, un singolo \textit{exit point} e nessun \textit{branch} interno.
\end{definit1}

I \textit{basic block} si identificano facilmente poiché iniziano con un \textit{leader} che può essere dei seguenti tipi:
\begin{itemize}
	\item l'\textit{entry point} del programma (il primo statement);
	\item ogni statement che è target di branch (condizionali o non condizionali) che contengono dei \textit{GoTo}
	\item ogni statement che segue un branch (condizionale o non condizionale) o un \textit{return}.
\end{itemize}

Dopo aver diviso il codice in \textit{basic block} (individuati tramite i \textit{leader} di ciascun blocco), essi verranno collegati dagli archi, in corrispondenza di:
\begin{itemize}
	\item \textit{GoTo} non condizionali;
	\item branch condizionali / archi multipli;
	\item flusso di programma (il controllo passa ad un altro blocco se non ci sono branch alla fine).
\end{itemize}

Se non c'è un unico \textit{entry-node} $n_0$ ed un unico \textit{exit-node} $n_f$, si aggiungono \textit{dummy nodes} e gli archi necessari (nessun arco entrante in $n_0$ e nessun arco uscende da $n_f$).

\subsection{Notazione dei CFG}
Dato un $CFG$ $=$ $<N, E>$:
\begin{itemize}
	\item Se c'è un arco $n_in_j\in E$:
	\begin{itemize}
		\item $n_i$ è \textit{predecessore} di $N_j$;
		\item $n_j$ è un \textit{successore} di $n_i$.
	\end{itemize}
	\item Per ogni nodo $n\in N$:
	\begin{itemize}
		\item textit{Pred(n)}: è l'insieme dei predecessori di $n$;
		\item textit{Succ(n)}: è l'insieme dei successori di $n$;
		\item un \textit{branch node} è un nodo che ha più di un successore;
		\item un \textit{join node} è un nodo che ha più di un predecessore; 
	\end{itemize}
\end{itemize}

\newtheorem*{definit2}{Depth First Traversal}
\begin{definit2}
	Il CFG è un grafo diretto e con radice (entry-node). Deve essere attraversato partendo dalla radice ed esplorando in profondità il più possibile ciascun ramo prima di fare backtracking.
	
	E' possibile costruire uno \textbf{spanning tree} per il grafo che contenga tutti i nodi, tale che:
	\begin{itemize}
		\item ci sia un percorso dalla radice ad ogni nodo che sia raggiungibile nel grafo originale;
		\item non devono esserci cicli.
	\end{itemize}
\end{definit2}

\noindent
I nodi vengono numerati nell'ordine in cui verranno visitati.
\newpage
\begin{multicols}{2}	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{pngs/SPT1}
	\end{figure}
	\columnbreak
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{pngs/SPT2}
	\end{figure}
\end{multicols}



\newtheorem*{definit3}{Classificazione degli archi}
\begin{definit3}
	Dato un arco $x \rightarrow y$ in un CFG, esso sarà:
	\begin{itemize}
		\item un \textbf{arco avanzante}: se $x$ è predecessore di $y$ nell'albero;
		\begin{itemize}
			\item \textbf{tree edge}: se è parte dello spanning tree;
			\item \textbf{forward edge}: se non è parte dello spanning tree e $x$ è predecessore di $y$ nell'albero.
		\end{itemize}
		\item un \textbf{arco all'indietro}: se $y$ è un predecessore di $x$ nell'albero;
		\item un \textbf{cross edge}: se non è parte dello spanning tree e nessun nodo è predecessore dell'altro.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.27]{pngs/Edges}
	\end{figure}
\end{definit3}

\newtheorem*{definit4}{Extended Basic Block}
\begin{definit4}
	Insieme massimo di nodi che non contiene nessun nodo di join (oltre all'entry node). Ha un solo ingresso e più uscite.
\end{definit4}

\newtheorem*{definit5}{Natural Loop}
\begin{definit5}
	Un Loop è un insieme di nodi strettamente connessi. Ha un unico ingresso (l'unico modo per visitarlo). Deve contenere un unico arco all'indietro per ripercorrere il loop.
	
	Un loop che non contiene altri loops è un \textit{inner loop}.
\end{definit5}
\noindent
Per trovare un loop all'interno di un grafo è sufficiente cercare gli archi all'indietro ($n\rightarrow d$).
\newline
Per costruire un loop si aggiunge $d$, si aggiunge $n$ (se $n\neq d$), si considera ogni nodo $m\neq d$ all'interno del loop (inserendo tutti i predecessori di $m$).
\newline

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.27]{pngs/NaturalLoop}
	\caption{Natural loops example}
\end{figure}

\newtheorem*{definit6}{Dominance}
\begin{definit6}
	Un nodo $d$ domina un nodo $n$ se ogni percorso dall'entry node del grafo fino a $n$ passa attraverso $d$ $(d$ \textit{dom} $n)$.
\end{definit6}
\begin{itemize}
	\item \textit{Dom(n)}: l'insieme dei dominatori del nodo $n$;
	\item ogni nodo domina se stesso: $n\in Dom(n)$;
	\item il nodo $d$ domina strettamente $n$ se $d\in Dom(n)$ e $d\neq n$;
	\item \textit{Dominance-based loop recognition}: la entry di un loop domina tutti i nodi interni al loop.
\end{itemize}

Ogni nodo $n$ ha un unico \textit{dominatore immediato} $m$ che è l'ultimo dominatore di $n$ su ogni percorso dall'entry node a $n$ $(m$ \textit{idom} $n)$, $m\neq n$.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{pngs/DominatorExample}
	\caption{Dominator example}
\end{figure}



\chapter{Analisi Statica}

\section{Introduzione}
L'obiettivo dell'analisi statica è quello di dire, osservando le proprietà semantica di un programma, se una certa proprietà vale o meno. Esistono diverse tipologie di analisi statica: \begin{itemize}
	\item Control flow Analysis;
	\item Data flow Analysis (distributive e non-distributive);
\end{itemize}

\section{Analisi sul CFG}
Viene generato un CFG per ogni procedura. Le analisi che vengono eseguite sono localizzate a 3 livelli: \begin{enumerate}
	\item \textbf{Locali al blocco}: sono eseguite all'interno di uno stesso \textit{basic block};
	\item \textbf{Intra-procedurali}: considerano il flusso di informazioni nel singolo CFG;
	\item \textbf{Inter-procedurali}: considerano il flusso di informazioni tra le procedure (con archi che rappresentano le chiamate di funzione).
\end{enumerate}

\noindent
L'analisi di \textit{data-flow} dice come l'informazione viene manipolata in un blocco. L'informazione è caratterizzata dalla soluzione dell'equazione di punto fisso definita per ogni blocco.

\noindent
In alcuni casi questa equazione è ottenuta in 3 passaggi: \begin{itemize}
	\item definendo l'informazione entrante in un blocco, che è l'unione dell'informazione di uscita del blocco precedente;
	\item definendo l'informazione in uscita dal blocco che è l'informazione in ingresso, modificata dalle operazioni eseguite nel blocco;
	\item queste definizioni vengono poi combinate nell'equazione del punto fisso.
\end{itemize}

Le analisi di \textit{data-flow} seguono il seguente schema:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.376]{pngs/FB}
\end{figure}

\newpage
\section{Soluzioni MFP - MOP - IDEAL}
Per le equazioni di \textit{data-flow analysis} esistono 3 tipi di soluzioni:
\begin{itemize}
	\item \textit{\textbf{MFP} (maximum fixed point)}: è la soluzione che combina i valori dell'analisi quando il CFG ha dei nodi in cui convergono due o più percorsi; questa soluzione approssima la \textit{MOP}.
	\item \textit{\textbf{MOP} (merge over all paths)}: è la soluzione più precisa rispetto alla \textit{MFP} ($MOP \sqsupseteq MFP$) poiché combina i valori dell'analisi di tutti i possibili percorsi del CFG dopo averli attraversati tutti. In generale, questa soluzione non è computabile perché ci posso essere un numero esponenziale (o infinito) di percorsi possibili:
	\begin{itemize}
		\item loop con guardia sempre vera;
		\item un programma che contiene $N$ \textit{if} statement avrà $2^N$ percorsi di esecuzione;
	\end{itemize}
	\item \textbf{\textit{IDEAL}}: è la soluzione migliore ma non è computabile. A differenza della \textit{MOP}, prende in considerazione solamente i percorsi che verrano attraversati sicuramente da almeno qualche esecuzione. Calcola il valore alla fine di ogni possibile percorso di esecuzione e calcola poi il \textit{meet} di questi valori.
	\begin{itemize}
		\item ogni soluzione più grande di \textit{IDEAL} è scorretta;
		\item ogni soluzione più piccola di \textit{IDEAL} è conservativa (\textit{safe});
	\end{itemize}
\end{itemize}
Se la funzione di trasferimento di ogni arco è \textbf{\textit{distributiva}} ($f(x \cup y) = f(x) \cup f(y)$) (e ogni program point è raggiungibile dall'entry point), allora la soluzione delle equazioni di \textit{data-flow} è la stessa per \textit{MOP} e \textit{MFP} ($MOP = MFP$). Dunque per le funzioni di trasferimento distributive, è possibile calcolare la soluzione \textit{MOP} attraverso l'algoritmo iterativo del punto fisso.
\newline

I \textbf{problemi \textit{distributivi}} sono i cosiddetti problemi "\textit{semplici}", come ad esempio: \textit{\textbf{live variables}}, \textit{\textbf{available expressions}}, \textit{\textbf{reaching definitions}} e \textit{\textbf{very busy expressions}} (tutte proprietà che ci dicono \textit{COME} un programma viene eseguito).

I \textbf{problemi \textit{non-distributivi}} sono quelli che ci dicono \textit{COSA} calcola un programma (ad esempio che l'output è costante, valori positivi, intervalli etc.). Un esempio di problema non distributivo è la \textit{\textbf{constant propagation analysis}}.

\begin{multicols}{2}	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{pngs/MOPMFP}
	\end{figure}
	\columnbreak
	Se consideriamo la \textit{constant propagation}, in questo programma il valore ci $C$ sarà sempre $5$, indipendentemente dal valore della guardia dello statement \textit{if}.
	
	Con una soluzione \textit{MFP}, $C$ non verrà mai considerata una costante, al contrario, con una soluzione \textit{MOP} otterremo come informazione che la variabile $C$ è una costante.
\end{multicols}

\newpage
\section{Data Flow Analysis}
Insieme di tecniche che raccolgono informazione su come i dati fluiscono durante l'esecuzione.

\subsection{Available Expressions}
L'espressione $e$ è \textit{available} se è valutata e assegnata ad una variabile prima di $v$ (uso della variabile). Tra la valutazione e $v$ non vengono ridefinite le variabili dell'espressione e \textit{x} (\textit{x:=e}).
\newline

\noindent
\textit{\underline{Proprietà}}: Forward \& Definite
\newline

\noindent
\underline{\textit{Punto fisso:}}
\begin{align*}
	AvailIn(n) &= 
	\begin{cases}
		\emptyset &\text{ se } n = n_0 \\
		\bigcap_{m\in pred(n)} AvailOut(m) &\text{ altrimenti}
	\end{cases}\\ \\
	AvailOut(n) &= Gen(n) \cup (AvailIn(n)\backslash Kill(n))\\
	AvailIn(n) &= \bigcap_{m\in pred(n)} Gen(m) \cup (AvailIn(m)\backslash Kill(m))
\end{align*}

\noindent
\underline{\textit{Semantica:}}

Dominio astratto = Ass = \{assegnamenti $x\leftarrow e ~|~ x\notin Var(e)$\}

$A\subseteq Ass$
\begin{align*}
	\llbracket ; \rrbracket^\sharp A &= A\\
	\llbracket NonZero(e) \rrbracket^\sharp A &= \llbracket Zero(e) \rrbracket^\sharp A = A\\
	\llbracket x\leftarrow e \rrbracket^\sharp A &= \begin{cases}
		(A\backslash Occ(x)) \cup \{x\leftarrow e \} &\text{ se } x\notin Var(e) \\
		A\backslash Occ(x) &\text{ altrimenti}
	\end{cases}\\
	\llbracket x\leftarrow M[e] \rrbracket^\sharp A &= A\backslash Occ(x)\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp A &= A
\end{align*}

$Occ(x) = $ \{Assegnamenti che coinvolgono $x$ a destra o a sinistra\}

$Gen(n) = $ \{espressioni valutate nel blocco $n$ e nessun operando di $e$ è definito nuovamente tra l'ultima valutazione di $e$ in $n$ e la fine di $n$\}

$Kill(n) = $ \{espressioni uccise da una nuova definizione di $n$\}

\noindent


\newpage
\subsection{Very Busy Expressions}
Un assegnamento è \textit{busy} su un cammino $\pi$ se $\pi = \pi_1 ~k~ \pi_2$ con:
\begin{itemize}
	\item $k$ è un assegnamento $x\leftarrow e$;
	\item $\pi_1$ non contiene usi di $x$;
	\item $\pi_2$ non contiene modifiche di $\{x\}\cup Var(e)$.
\end{itemize}

\noindent
Un assegnamento è \textit{very busy} se è \textit{busy} su ogni percorso da $v$ a \textit{exit}.

\noindent
Dice come e quali espressioni anticipare.

\noindent
Un assegnamento è ucciso in un blocco $n$ se una delle sue variabili è modificata o se $e$ viene usata.

\noindent
Un assegnamento è generato in un blocco $n$ se si trova nel blocco e l'espressione non contiene la variabile che si sta assegnando.
\newline

\noindent
\textit{\underline{Proprietà}}: Backward \& Definite
\newline

\noindent
\underline{\textit{Punto fisso:}}
\begin{align*}
	VB_{exit}(p) &= 
	\begin{cases}
		\emptyset &\text{ se } p = v_{exit} \\
		\bigcap_{q\in succ(p)} VB_{entry}(q) &\text{ altrimenti}
	\end{cases}\\ \\
	VB_{entry}(p) &= Gen(p) \cup (VB_{exit}(p)\backslash Kill(p))\\
	VB_{exit}(p) &= \bigcap_{q\in succ(p)} Gen(q) \cup (VB_{exit}(q)\backslash Kill(q))
\end{align*}

\noindent
\underline{\textit{Semantica:}}

$B = 2^{Ass} = \mathcal{P}(Ass)$
\begin{align*}
	\llbracket ; \rrbracket^\sharp B &= B\\
	\llbracket NonZero(e) \rrbracket^\sharp B &= \llbracket Zero(e) \rrbracket^\sharp B = B \backslash Ass(e)\\
	\llbracket x\leftarrow e \rrbracket^\sharp B &= \begin{cases}
		B\backslash (Occ(x) \cup Ass(e)) \cup \{x\leftarrow e \} &\text{ se } x\notin Var(e) \\
		B\backslash (Occ(x) \cup Ass(e)) &\text{ altrimenti}
	\end{cases}\\
	\llbracket x\leftarrow M[e] \rrbracket^\sharp B &= B\backslash (Occ(x) \cup Ass(e))\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp B &= B\backslash (Ass(e_1) \cup Ass(e_2))
\end{align*}

$Use(n) = $ \{occorrenza di una variabile sul lato destro di uno statement\}

\newpage
\subsection{Liveness}
$x$ è \textit{live} all'uscita del blocco $b$ se verrà usata successivamente. $x$ non è \textit{live} o (\textit{dead}) se viene ridefinita prima di un successivo uso.

\noindent
$x$ è \textit{live} in un cammino $\pi$ ($v\rightarrow exit$) se:
\begin{itemize}
	\item $\pi$ non contiene \textit{Def(x)} e,
	\item esiste almeno un uso di $x$ in $\pi$ che segue la \textit{Def(x)};
\end{itemize}
\noindent
$x$ è \textit{live} se si trova tra una definizione ed un uso.

\noindent
Dice se $a$ e $b$ possono essere memorizzate nella stessa locazione, cioè se $a$ e $b$ non sono mai \textit{live} insieme, allora posso sostituire $a$ con $b$.

\begin{itemize}
	\item $x\in Use(n) \Rightarrow$ x \textit{LiveIn} in $n$
	\item $x$ è \textit{LiveOut} in $n$ e $x\notin$ \textit{VarKill(n)} $\Rightarrow$ $x$ \textit{LiveIn} in $n$;
	\item $x$ è \textit{LiveIn} in almeno un \textit{Succ(n)} $\Rightarrow$ $x$ \textit{LiveOut(n)};
\end{itemize}

\noindent
\textit{\underline{Falsi positivi}}:
\begin{itemize}
	\item $x$ è accessibile attraverso altri nomi $\Rightarrow$ Liveness fallisce;
	\item analizzi anche cammini non possibili;
	\item inizializzazione in altre procedure (perché questa analisi è intra-procedurale);
\end{itemize}

\noindent
\textit{\underline{Proprietà}}: Backward \& Possible
\newline

\noindent
\underline{\textit{Punto fisso:}}
\begin{align*}
	LiveOut(n) &=
	\begin{cases}
		\emptyset &\text{ se $n= exit$} \\
		\bigcup_{m\in Succ(n)} LiveIn(m) &\text{ altrimenti}
	\end{cases}\\
	LiveIn(n) &= Use(n) \cup (LiveOut(n)\backslash VarKill(n))\\
	LiveOut(n) &= \bigcup_{m\in Succ(n)} Use(m) \cup (LiveOut(m)\backslash VarKill(m))
\end{align*}

\noindent
\underline{\textit{Semantica:}}

Dominio astratto = $\mathcal{P}(Var)$

$L\subseteq Var$
\begin{align*}
	\llbracket ; \rrbracket^\sharp L &= L\\
	\llbracket NonZero(e) \rrbracket^\sharp L &= \llbracket Zero(e) \rrbracket^\sharp L = L \cup Var(e)\\
	\llbracket x\leftarrow e \rrbracket^\sharp L &= Var(e) \cup (L\backslash\{x\})\\
	\llbracket x\leftarrow M[e] \rrbracket^\sharp L &= Var(e) \cup (L\backslash\{x\})\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp L &= L \cup Var(e_1) \cup Var(e_2)
\end{align*}

$LiveIn(n) = $ \{sono le variabili \textit{live} in $n$ che sono \textit{live} su almeno un arco entrante\}

$LiveOut(n) = $ \{sono le variabili \textit{live} in $n$ che sono \textit{live} su almeno un arco uscente\}

$VarKill(n) = Def(n)$, cioè le definizioni presenti in n
\newline

\noindent
\underline{\textit{True Liveness:}} un \textit{true use} è un uso in un assegnamento ad una variabile \textit{live}. Se assegno $x$ ad una variabile \textit{non-live}, allora anche $x$ non è \textit{live}.

\newpage
\subsection*{Copy Propagation}
L'analisi ad ogni program point tiene traccia delle copie di $x$.

\noindent
Se ho un assegnamento $T\leftarrow x+1$ e poi $y\leftarrow T$, allora quest'ultimo è inutile.
\newline

\noindent
\textit{\underline{Proprietà}}: Forward \& Definite
\newline

\noindent
\underline{\textit{Punto fisso:}}
\begin{align*}
	Copie_{entry}(n) &= \bigcap_{m\in Pred(n)} Copie_{exit}(m)\\
	Copie_{exit}(n) &= \bigcap_{m\in Pred(n)} Gen(m) \cup (Copie_{exit}(m)\backslash Kill(m))
\end{align*}

\noindent
\underline{\textit{Semantica:}}

Dominio astratto = $\mathcal{V}_x$ = $\{V \subseteq Var ~|~ x\in V\}$ perché $x$ è copia di se stesso.

$V\subseteq Var$

Entry $V_0=\{x\}$ perché $x$ è copia di se stesso e cerco le altre sue copie.
\begin{align*}
	\llbracket ; \rrbracket^\sharp V &= V\\
	\llbracket NonZero(e) \rrbracket^\sharp V &= \llbracket Zero(e) \rrbracket^\sharp V = V\\
	\llbracket x\leftarrow e \rrbracket^\sharp V &= \llbracket x\leftarrow M[e]\rrbracket^\sharp V = \{x\}\\
	\llbracket z\leftarrow y \rrbracket^\sharp V &=
	\begin{cases}
		V \cup \{z\} &\text{ se } y\in V \text{(y è copia di x)} \\
		V\backslash \{z\} &\text{ altrimenti}
	\end{cases}\\
	\llbracket y\leftarrow e \rrbracket^\sharp V &= V\backslash \{y\}\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp V &= V
\end{align*}

$Gen(n) = \{(x == y) ~|~ n$ contiene $x\leftarrow y \}$

$Kill(n) = \{(x == y) ~|~ x$ è ridefinita in $n \}$

\subsection{Reaching Definition}
Dato un program point $p$ vogliamo identificare le definizioni di variabili che raggiungono $p$.

\noindent
Viene usata in \textit{code motion}: se uso un assegnamento in tutto il ciclo senza modificarlo, allora lo sposto all'entrata del ciclo.
\newline

\noindent
\textit{\underline{Proprietà}}: Forward \& Possible
\newline

\noindent
\underline{\textit{Punto fisso (non c'è la semantica):}}
\begin{align*}
	RD_{entry}(n) &=
	\begin{cases}
		i=\{(x, ?) ~|~ x\in Var \} &\text{ se $n= entry$} \\
		\bigcup_{m\in Pred(n)} RD_{exit}(m) &\text{ altrimenti}
	\end{cases}\\
	RD_{exit}(n) &= Gen(n) \cup (RD_{entry}(n)\backslash Kill(n))
\end{align*}

$\{(x, p) ~|~ x\in Vars$, $p$ punto di programma$\}$

\textit{Inizializzazione}: $i=\{(x, ?) ~|~ x\in Vars$, variabile non inizializzata)  $\}$

$Gen(n) =$ \{definizioni $(x, l)$ dentro $n$ e disponibili alla fine di $n$ \}

$Kill(n) = \{(x,p) ~|~ x$ è ridefinita in $n \}$

\newpage
\subsection{Riepilogo}

\begin{center}
	\begin{tabular}{c|c|c}
		&\thead{\textbf{Possible $(\bigcup)$}}&\textbf{Definite $(\bigcap)$}\\ \hline
		\textbf{Forward}&Reaching Definition&\makecell{Available Expr, \\Copy Propagation}\\ \hline
		\makecell{\textbf{Backward}\\}&Liveness&Very Busy Expr\\
	\end{tabular}
\end{center}

\textit{\underline{Available Expressions}}:
\begin{align*}
	AvailIn(n) &= 
	\begin{cases}
		\emptyset &\text{ se } n = n_0 \\
		\bigcap_{m\in pred(n)} AvailOut(m) &\text{ altrimenti}
	\end{cases}\\ \\
	AvailOut(n) &= Gen(n) \cup (AvailIn(n)\backslash Kill(n))
\end{align*}
\\

\textit{\underline{Very Busy}}:
\begin{align*}
	VB_{exit}(p) &= 
	\begin{cases}
		\emptyset &\text{ se } p = v_{exit} \\
		\bigcap_{q\in succ(p)} VB_{entry}(q) &\text{ altrimenti}
	\end{cases}\\ \\
	VB_{entry}(p) &= Gen(p) \cup (VB_{exit}(p)\backslash Kill(p))
\end{align*}
\\

\textit{\underline{Liveness}}:
\begin{align*}
	LiveOut(n) &=
	\begin{cases}
		\emptyset &\text{ se $n= exit$} \\
		\bigcup_{m\in Succ(n)} LiveIn(m) &\text{ altrimenti}
	\end{cases}\\
	LiveIn(n) &= Use(n) \cup (LiveOut(n)\backslash VarKill(n))
\end{align*}
\\

\textit{\underline{Reaching Definition}}:
\begin{align*}
	RD_{entry}(n) &=
	\begin{cases}
		i=\{(x, ?) ~|~ x\in Var \} &\text{ se $n= entry$} \\
		\bigcup_{m\in Pred(n)} RD_{exit}(m) &\text{ altrimenti}
	\end{cases}\\
	RD_{exit}(n) &= Gen(n) \cup (RD_{entry}(n)\backslash Kill(n))
\end{align*}


\newpage
\section{Problemi Non-Distributivi}

\subsection{Costanti}
Ogni singoletto non è confrontabile con gli altri. Se una costante assume due valori va in $\top$. \MakeUppercase{è} un reticolo completo poiché contiene $\emptyset$ ed è \textit{ACC} perché è finito in altezza.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{pngs/Const}
\end{figure}

Dominio concreto: $\mathbb{V}\rightarrow \mathbb{Z}$

Dominio astratto: $\mathbb{V}\rightarrow \mathcal{P}(\mathbb{Z})$
\newline

\noindent
\underline{\textit{Semantica astratta delle espressioni:}}
\begin{align*}
	op &= operatore\\
	a ~op~ b &=
	\begin{cases}
		a ~op~ b &\text{ se a e b sono costanti}\\
		\top &\text{ se $a=\top$ $\lor~$ $b=\top$}
	\end{cases}\\
	\llbracket c \rrbracket^\sharp D &= c\\
	\llbracket op~e \rrbracket^\sharp D &= op^\sharp\llbracket e\rrbracket^\sharp D\\
	\llbracket e_1~op~e_2 \rrbracket^\sharp D &= \llbracket e_1\rrbracket^\sharp D ~op^\sharp~\llbracket e_2\rrbracket^\sharp D\\
	\llbracket x\rrbracket^\sharp D &= D(x)
\end{align*}

\underline{\textit{Semantica astratta dei comandi:}}

$D =$ memoria
\begin{align*}
	\llbracket ; \rrbracket^\sharp D &= D\\
	\llbracket NonZero(e) \rrbracket^\sharp D &=
	\begin{cases}
		\bot &\text{ se } \llbracket e\rrbracket^\sharp D = 0 \\
		D &\text{ se } \llbracket e\rrbracket^\sharp D \neq 0
	\end{cases}\\
	\llbracket Zero(e) \rrbracket^\sharp D &=
	\begin{cases}
		D &\text{ se } 0\subseteq \llbracket e\rrbracket^\sharp D \\
		\bot &\text{ se } 0\not\subseteq \llbracket e\rrbracket^\sharp D
	\end{cases}\\
	\llbracket x\leftarrow e \rrbracket^\sharp D &= D[x\mapsto \llbracket e\rrbracket^\sharp D]\\
	\llbracket x\leftarrow M[e] \rrbracket^\sharp D &= D[x\mapsto \top] \text{\hspace{0.5cm} non so cos'è M[e] perché lo valuterò dopo}\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp D &= D
\end{align*}

\subsection{Segni}
Dominio rappresentato da un semipiano (un insieme di punti), quindi non va subito a $\top$.

\newpage
\subsection{Intervalli}
Il dominio degli Intervalli non è \textit{ACC}, dunque non garantisce la terminazione: per questo viene introdotto il \textit{widening}.

\begin{center}
	$[a, b]$ dove $a\leq x\leq b$ (convessi)
	
	$\mathbb{I} = \{[l, u] ~|~ l\in \mathbb{Z}\cup \{-\infty\}, u\in\mathbb{Z}\cup \{+\infty\}, ~l\leq u\}$
\end{center}

\noindent
\textit{\underline{Semantica astratta delle espressioni:}}
\begin{itemize}
	\item $[l_1, u_1] +^\sharp [l_2, u_2] = [l_1 + l_2, u_1 + U_2]$
	\item $-^\sharp[l, u] = [-u, -l]$
	\item $[l_1, u_1] *^\sharp [l_2, u_2] = [a, b]$ dove:
	\begin{itemize}
		\item $a=min(l_1*l_2, l_1*u_2, l_2*u_1, l_2*u_2)$
		\item $b=max(l_1*l_2, l_1*u_2, l_2*u_1, l_2*u_2)$
	\end{itemize}
	\item $[l_1, u_1] =^\sharp [l_2, u_2] =
	\begin{cases}
	[1, 1] &\text{ se } l_1=l_2=u_1=u_2 (costanti) \\
	[0, 0] &\text{ se } u_1 < l_2 \lor ~u_2 < l_1\\
	[0, 1] &\text{ altrimenti (intervalli uguali che approssimano valori diversi) }
	\end{cases}$
	\item $[l_1, u_1] <^\sharp [l_2, u_2] =
	\begin{cases}
	[1, 1] &\text{ se } u_1<l_2 \\
	[0, 0] &\text{ se } u_2 \leq l_1 \\
	[0, 1] &\text{ altrimenti }
	\end{cases}$
\end{itemize}

\underline{\textit{Semantica astratta dei comandi:}}

$D : \mathbb{V} \rightarrow \mathbb{I}$
\begin{align*}
	\llbracket ; \rrbracket^\sharp D &= D\\
	\llbracket NonZero(e) \rrbracket^\sharp D &=
	\begin{cases}
		\bot &\text{ se } [0, 0] = \llbracket e \rrbracket^\sharp D \\
		D &\text{ se } [0, 0] \neq \llbracket e \rrbracket^\sharp D~ \text{ (contiene anche altri valori)}
	\end{cases}\\
	\llbracket Zero(e) \rrbracket^\sharp D &=
	\begin{cases}
		D &\text{ se } [0, 0] \subseteq \llbracket e \rrbracket^\sharp D~ \text{ (lo $0$ è uno dei possibili valori)} \\
		\bot &\text{ se } [0, 0] \not \subseteq \llbracket e \rrbracket^\sharp D~ \text{ (l'intervallo $e$ non contiene $0$)}
	\end{cases}\\
	\llbracket x\leftarrow e \rrbracket^\sharp D &= D[x\mapsto \llbracket e\rrbracket^\sharp D]\\
	\llbracket x\leftarrow M[e] \rrbracket^\sharp D &= D[x\mapsto [-\infty, +\infty]~~ \text{ (elemento $\top$ degli intervalli)}\\
	\llbracket M[e_1]\leftarrow e_2 \rrbracket^\sharp D &= D
\end{align*}




\chapter{Analisi Dinamica}
L'analisi dinamica di un programma si basa sulla sua esecuzione e viene utilizzati in vari ambiti: \textit{testing}, \textit{debugging}, \textit{emulation/virtualization}, \textit{profiling/tracing}, \textit{monitoring}, \textit{dynamic slicing}.

Nelle sezioni seguenti ne analizziamo alcuni nel dettaglio.

\section{Testing}
Si tratta principalmente dell'esecuzione di un programma basata su un campione di dati (molto piccolo) passato come input.

L'\textbf{obiettivo} è la ricerca di bug/errori/difetti del software, senza correggerli. Questa operazione viene svolta nella fase di testing da professionisti con un'esperienza nella ricerca e identificazione dei bug.
\newline
\newline
\noindent
Durante la fase di testing si devono ricercare:
\begin{itemize}
	\item \textbf{mistake}: un'azione umana che ha prodotto un risultato scorretto;
	\item \textbf{fault}: un passaggio scorretto (una definizione di variabile...) all'interno del programma;
	\item \textbf{failure}: la mancata abilità da parte del sistema di svolgere le funzioni richieste;
	\item \textbf{errori}: la differenza tra il valore atteso e il valore effettivamente calcolato/osservato;
	\item \textbf{specifiche}: un documento che specifica, in modo completo e preciso, le richieste e le caratteristiche del sistema e/o dei componenti e spesso delle procedure per verificare quali delle disposizioni sono state soddisfatte.
\end{itemize}

\section{Debugging}

L'\textbf{obiettivo} è l'identificazione, l'isolamento e la risoluzione dei problemi/bug. Questa operazione si può svolgere durante la fase di sviluppo del software oppure in una fase apposita in cui vengono sistemati i bug riportati dopo i test.

\section{Program Slicing}
Si tratta di una tecnica di decomposizione che trasforma un programma originale, cancellandone alcune istruzioni che non hanno alcun effetto sulle \textit{variabili di interesse} nei \textit{punti di interesse}.

Lo \textit{slice} è il programma trasformato secondo il \textit{criterio di slicing} che descrive i parametri di interesse: \textit{V} (insieme delle variabili di interesse) e \textit{n} (punti di interesse del programma).

Ci sono diversi motivi per i quali effettuare il \textit{program slicing}: \textit{program debugging}, \textit{testing} (lo slicing riduce i costi del \textit{regresssion testing} dopo una trasformazione del codice), \textit{parallelizzazione},	\textit{compresione di una programma} (effettuare lo slicing aiuta a comprendere come viene eseguito un programma e quali variabili verranno modificate nei vari percorsi) e \textit{mantenimento del software} (per modificare il codice senza \textit{side effects} indesiderati in giro per il programma).
\newline
\newline
\noindent
Esistono \textbf{diversi tipi di program slicing}:
\begin{itemize}
	\item \textit{\textbf{Static slicing}}: l'equivalenza tra programma originale e slice deve, implicitamente, essere valida per ogni possibile input;
	\item \textit{\textbf{Conditioned slicing}}: preserva il significato del programma originale per un insieme di input che soddisfa una particolare condizione $\phi$;
	\item \textit{\textbf{Dynamic slicing}}: considera una particolare computazione, e dunque un particolare input, in modo da preservare il significato del programma unicamente per quell'input.
\end{itemize}

\noindent
Esistono, inoltre, \textbf{diverse forme di program slicing}:
\begin{itemize}
	\item \textit{\textbf{Korel \& Laski} (KL)}: è una forma di slicing molto forte in cui il programma e lo slice devono seguire \textit{paths} identici. Il programma e lo slice hanno la stessa semantica operazionale. Il \textit{path} seguito dallo slice deve essere un \textit{subpath} dell'esecuzione originale.
	\item \textit{\textbf{Iteration Count} (IC)}: richiede che lo slice e il programma si pareggino solo ad una certa iterazione $k$ di un program point $n$ (cioè quando lo statement al program point $n$ viene eseguito per la $k$-esima volta), e non per tutte le iterazioni dello stesso program point.
	\item \textit{\textbf{KL-IC}}(combinazione dei precedenti): richiede che il programma e lo slice seguano \textit{paths} identici e siano uguali solamente ad una particolare iterazione di un certo program point.
\end{itemize}
	
	
	
	
	
	
	
	
	
	
	
\end{document}